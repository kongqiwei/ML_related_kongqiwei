##算法思想：决策树可以理解为就是if-else的组合，节点为判别条件，叶子节点为最终分类结果，其中ID3、C4.5,、CART算法根据不同的规则解决不同的问
#ID3主要通过信息熵进行判别划分，C4.5为了解决ID3对多分类偏好缺陷问题而改进为通过信息增益比进行判别规则划分，而CART则为了降低负责
#降低复杂度和避免过拟合问题而采取预剪枝和后剪枝两种方法进行改进。
from sklearn.datasets import load_iris #导入iris数据集
import numpy as np
import math
from collections import Counter #对数据样本进行计数
import matplotlib.pyplot as plt
from pylab import *

class decisionnode:   #定义节点类的结构
    def __init__(self, d=None, thre=None, results=None, NH=None, lb=None, rb=None, max_label=None):
        self.d = d   # d表示维度
        self.thre = thre  # thre：二分时的比较判别界限值，将样本集分为2类
        self.results = results  # 最后的叶节点代表的类别
        self.NH = NH  # 存储各节点的样本量与经验熵的乘积，便于剪枝时使用
        self.lb = lb  # desision node,对应于样本在d维的数据小于thre时,左节点
        self.rb = rb  # desision node,对应于样本在d维的数据大于thre时，右节点
        self.max_label = max_label  # 记录当前节点包含的label中同类最多的label，便于投票处理

def entropy(y):
    #计算信息熵，y为labels，此处最终的categor为list结构
    if y.size > 1:
        category = list(set(y))  #对原列表去重并按从小到大排序，对集合排序
    else:
        category = [y.item()]   # items() 函数以列表返回可遍历的(键, 值) 元组数组。
        y = [y.item()]
    ent = 0
    for label in category:
        p = len([label_ for label_ in y if label_ == label]) / len(y)    #求一个label在总的label中的比例，即概率
        ent += -p * math.log(p, 2)
    return ent

def Gini(y):
    #计算labels的基尼指数，y为labels
    category = list(set(y))
    gini = 1
    for label in category:
        p = len([label_ for label_ in y if label_ == label]) / len(y)
        gini += -p * p  #基尼指数和熵的定义是很不一样的，这也是三个不同算法的区别
    return gini

def GainEnt_max(X, y, d):
    #计算选择属性attr的最大信息增益，X为样本集,y为label，d为一个维度（即列数，属于后面的传入参数，type为int）
    ent_X = entropy(y)   #计算特征的熵，可以回忆西瓜书中不同的西瓜特征划分那个计算过程
    X_attr = X[:, d]     #取出样本集中所有行的第d列数据，即d维数据
    X_attr = list(set(X_attr))   #转化为list结构
    X_attr = sorted(X_attr)
    Gain = 0
    thre = 0
    for i in range(len(X_attr) - 1):
        thre_temp = (X_attr[i] + X_attr[i + 1]) / 2   #将平均值作为划分依据
        y_small_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] <= thre_temp]  #左节点索引
        y_big_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] > thre_temp]  #右节点索引
        y_small = y[y_small_index]   #左节点，这是一个list结构，索引是变化的，存储了所有的划到左节点的样本集
        y_big = y[y_big_index]
        Gain_temp = ent_X - (len(y_small) / len(y)) * entropy(y_small) - (len(y_big) / len(y)) * entropy(y_big)
        if Gain < Gain_temp:   #计算信息增益
            Gain = Gain_temp
            thre = thre_temp   #迭代新的划分值
    return Gain, thre  #返回最佳划分属性和划分值

def Gini_index_min(X, y, d):
    #计算选择属性attr的最小基尼指数，X为样本集,y为label，d为一个维度，type为int
    X = X.reshape(-1, len(X.T))  #-1为最后一列，即属性列
    X_attr = X[:, d]     #取出样本集中所有行的第d列数据，即d维数据
    X_attr = list(set(X_attr))
    X_attr = sorted(X_attr)
    Gini_index = 1
    thre = 0
    for i in range(len(X_attr) - 1):
        thre_temp = (X_attr[i] + X_attr[i + 1]) / 2
        y_small_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] <= thre_temp]
        y_big_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] > thre_temp]
        y_small = y[y_small_index]
        y_big = y[y_big_index]
        Gini_index_temp = (len(y_small) / len(y)) * Gini(y_small) + (len(y_big) / len(y)) * Gini(y_big)
        if Gini_index > Gini_index_temp:  #迭代选择最佳属性索引和属性划分值
            Gini_index = Gini_index_temp
            thre = thre_temp
    return Gini_index, thre  #返回索引和划分值

def attribute_based_on_GainEnt(X, y):
    #基于信息增益选择最优属性，X为样本集，y为labels
    D = np.arange(len(X[0]))  # np.arange()方法为创建一个等差数组，此处为返回样本集行数长度大小的自然数组
    Gain_max = 0
    thre_ = 0
    d_ = 0
    for d in D:
        Gain, thre = GainEnt_max(X, y, d)
        if Gain_max < Gain:
            Gain_max = Gain
            thre_ = thre
            d_ = d  # 返回维度，即第几列
    return Gain_max, thre_, d_

def attribute_based_on_Giniindex(X, y):
    #基于信息增益选择最优属性，X为样本集，y为label
    D = np.arange(len(X.T))
    Gini_Index_Min = 1
    thre_ = 0
    d_ = 0
    for d in D:
        Gini_index, thre = Gini_index_min(X, y, d)
        if Gini_Index_Min > Gini_index:
            Gini_Index_Min = Gini_index
            thre_ = thre
            d_ = d  # 维度标号
    return Gini_Index_Min, thre_, d_

def devide_group(X, y, thre, d):
    #按照维度d下阈值为thre分为两类并返回，即节点的具体划分
    X_in_d = X[:, d]
    x_small_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] <= thre]
    x_big_index = [i_arg for i_arg in range(len(X[:, d])) if X[i_arg, d] > thre]
    X_small = X[x_small_index]  #左节点中的样本数据
    y_small = y[x_small_index]   #左节点中样本的labels
    X_big = X[x_big_index]
    y_big = y[x_big_index]
    return X_small, y_small, X_big, y_big

def NtHt(y):
    #计算经验熵与样本数的乘积，用来剪枝，y为labels
    ent = entropy(y)
    print('entropy={},labels_len={},all_entropy={}'.format(ent, len(y), ent * len(y)))#str.format()格式化函数
    return ent * len(y)     #剪枝时参照是信息熵的比例，所以不同的label划分所得熵乘以对应label划分的划分类别数

def maxlabel(y):
    label_ = Counter(y).most_common(1) #most_common()函数用来实现Top n 功能.
    return label_[0][0]  #返回出现最多的label,投票原则的经典代码

def buildtree(X, y, method='Gini'):  #默认ID3算法，可以根据设置选择CART算法
    #递归的方式构建决策树
    if y.size > 1:  #不止一个label
        if method == 'Gini':  #ID3
            Gain_max, thre, d = attribute_based_on_Giniindex(X, y)
        elif method == 'GainEnt': #CART
            Gain_max, thre, d = attribute_based_on_GainEnt(X, y)
        if (Gain_max > 0 and method == 'GainEnt') or (Gain_max >= 0 and len(list(set(y))) > 1 and method == 'Gini'):
            X_small, y_small, X_big, y_big = devide_group(X, y, thre, d)
            left_branch = buildtree(X_small, y_small, method=method)   #递归生成左右子树
            right_branch = buildtree(X_big, y_big, method=method)
            nh = NtHt(y)  #如果是CART,则要剪枝
            max_label = maxlabel(y)   #当前节点生成
            return decisionnode(d=d, thre=thre, NH=nh, lb=left_branch, rb=right_branch, max_label=max_label)
        else:
            nh = NtHt(y)
            max_label = maxlabel(y)  #叶子节点
            return decisionnode(results=y[0], NH=nh, max_label=max_label)
    else:
        nh = NtHt(y)
        max_label = maxlabel(y)   #如果只有一个label则是叶子
        return decisionnode(results=y.item(), NH=nh, max_label=max_label)

def printtree(tree, indent='--', dict_tree={}, direct='Left'):
    # 是否是叶节点
    if tree.results != None:
        print(tree.results)
        dict_tree = {direct: str(tree.results)}
    else:
        # 打印判断条件
        print(str(tree.d) + ":" + str(tree.thre) + "? ")
        # 打印分支
        print(indent + "Left->",)
        a = printtree(tree.lb, indent=indent + "--", direct='Left')
        aa = a.copy()
        print(indent + "Right->",)
        b = printtree(tree.rb, indent=indent + "--", direct='Right')
        bb = b.copy()
        aa.update(bb)
        stri = str(tree.d) + ":" + str(tree.thre) + "?"
        if indent != '--':
            dict_tree = {direct: {stri: aa}}
        else:
            dict_tree = {stri: aa}
    return dict_tree

def classify(observation, tree):   #打印时候左右分类
    if tree.results != None:       #该函数在后面的预测准确率计算时候使用
        return tree.results
    else:
        v = observation[tree.d]
        branch = None
        if v > tree.thre:  #往左还是右
            branch = tree.rb
        else:
            branch = tree.lb
        return classify(observation, branch)  #递归打印左右节点

def pruning(tree, alpha=0.1):  #剪枝函数
    if tree.lb.results == None:
        pruning(tree.lb, alpha)  #递归剪枝
    if tree.rb.results == None:
        pruning(tree.rb, alpha)
    if tree.lb.results != None and tree.rb.results != None:
        before_pruning = tree.lb.NH + tree.rb.NH + 2 * alpha  #剪枝前的效果，左右节点中的信息增益比
        after_pruning = tree.NH + alpha             #剪枝后效果，节点的信息增益比
        print('before_pruning={},after_pruning={}'.format(before_pruning, after_pruning))
        if after_pruning <= before_pruning: #剪枝与否的判断
            print('pruning--{}:{}?'.format(tree.d, tree.thre))
            tree.lb, tree.rb = None, None
            tree.results = tree.max_label  #返回剪枝后的label,还是投票原则


decisionNode = dict(boxstyle="round4", color='#3366FF')  # 定义判断结点形态
leafNode = dict(boxstyle="circle", color='#FF6633')  # 定义叶结点形态
arrow_args = dict(arrowstyle="<-", color='g')  # 定义箭头
# 绘制带箭头的注释
def plotNode(nodeTxt, centerPt, parentPt, nodeType):
    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',
                            xytext=centerPt, textcoords='axes fraction',
                            va="center", ha="center", bbox=nodeType, arrowprops=arrow_args)

# 计算叶结点数
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':  #通过是否是字典结构判断是否是叶节点
            numLeafs += getNumLeafs(secondDict[key])  #递归判断
        else:
            numLeafs += 1
    return numLeafs

# 计算树的层数
def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = list(myTree.keys())[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':  #和求叶子节点个数一样的思想
            thisDepth = 1 + getTreeDepth(secondDict[key]) #递归下一个
        else:
            thisDepth = 1
        if thisDepth > maxDepth:
            maxDepth = thisDepth
    return maxDepth

# 在父子结点间填充文本信息
def plotMidText(cntrPt, parentPt, txtString):
    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]
    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]
    createPlot.ax1.text(xMid, yMid, txtString, va="center",
                        ha="center", rotation=30)

def plotTree(myTree, parentPt, nodeTxt):
    numLeafs = getNumLeafs(myTree)
    depth = getTreeDepth(myTree)
    firstStr = list(myTree.keys())[0]
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) /
              2.0 / plotTree.totalW, plotTree.yOff)
    plotMidText(cntrPt, parentPt, nodeTxt)  # 在父子结点间填充文本信息
    plotNode(firstStr, cntrPt, parentPt, decisionNode)  # 绘制带箭头的注释
    secondDict = myTree[firstStr]
    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD
    for key in secondDict.keys():
        if type(secondDict[key]).__name__ == 'dict':
            plotTree(secondDict[key], cntrPt, str(key))
        else:
            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW
            plotNode(secondDict[key], (plotTree.xOff,
                                       plotTree.yOff), cntrPt, leafNode)
            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD

def createPlot(inTree, index=1):
    fig = plt.figure(index, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)
    plotTree.totalW = float(getNumLeafs(inTree))
    plotTree.totalD = float(getTreeDepth(inTree))
    plotTree.xOff = -0.5 / plotTree.totalW
    plotTree.yOff = 1.0
    plotTree(inTree, (0.5, 1.0), '')

if __name__ == '__main__':
    iris = load_iris()
    X = iris.data    #以下代码设计数据集数据和类别分离的诸多操作，适用性很好
    y = iris.target   #iris的类别，即是哪种花
    permutation = np.random.permutation(X.shape[0])   #numpy.random.permutation(x)随机排列一个序列，或者数组
    shuffled_dataset = X[permutation, :]      #打乱iris的样本的顺序，让不同的类别随机分布
    shuffled_labels = y[permutation]
    train_data = shuffled_dataset[:100, :]  #训练集
    train_label = shuffled_labels[:100]
    test_data = shuffled_dataset[100:150, :]  #测试集
    test_label = shuffled_labels[100:150]
    tree1 = buildtree(train_data, train_label, method='Gini')  #ID3
    print('=============================')
    tree2 = buildtree(train_data, train_label, method='GainEnt')  #CART
    a = printtree(tree=tree1)
    b = printtree(tree=tree2)
    true_count = 0
    for i in range(len(test_label)):
        predict = classify(test_data[i], tree1)    #判断是否预测准确
        if predict == test_label[i]:
            true_count += 1
    print("numbers_of_True_in_50_samples_CART:{}".format(true_count))
    true_count = 0
    for i in range(len(test_label)):
        predict = classify(test_data[i], tree2)
        if predict == test_label[i]:
            true_count += 1
    print("numbers_of_True_in_50_samples_ID3:{}".format(true_count))
    mpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体
    mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像时负号'-'显示为方块的问题
    createPlot(a, 1)
    createPlot(b, 2)
    # 剪枝处理
    pruning(tree=tree1, alpha=4)
    pruning(tree=tree2, alpha=4)
    a = printtree(tree=tree1)
    b = printtree(tree=tree2)
    true_count = 0
    for i in range(len(test_label)):
        predict = classify(test_data[i], tree1)
        if predict == test_label[i]:
            true_count += 1
    print("numbers_of_True_in_50_samples_CART:{}".format(true_count))
    true_count = 0
    for i in range(len(test_label)):
        predict = classify(test_data[i], tree2)
        if predict == test_label[i]:
            true_count += 1
    print("numbers_of_True_in_50_samples_ID3:{}".format(true_count))
    createPlot(a, 3)
    createPlot(b, 4)
    plt.show()
