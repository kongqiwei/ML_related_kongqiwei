#PCA方法（主成分分析方法，PrincipalComponents Analysis)
#2019-10-04
#PCA主要是一个降维方法，因为很多的数据量很大，特征会很多，这样的话会导致（1）过拟合
（2）计算量大（3）存储复杂等问题
降维的思想为：用矩阵存储数据，使用特征值和特征向量表征数据，在这里，通过方差和协方差处理数据，因为方差代表了一个数据的特征，可以理解为因为方差越大所以这个特征越明显，就像人群中，最高的那个人最具高这个特征，而最胖那个人最具胖这个特征一样，所以通过计算方差处理数据，因为方差考虑的值差值，所以可以在数据的基础上减去均值。
一般来说，这里处理数据，都将列作为特征，而行作为一个数据样本。所以在处理时候涉及行列问题的处理，nump的优越性就有所体现。
#-end

from sklearn import datasets #仅仅用于导入iris数据集
import numpy  as np
def pca(X,k):   #k是成分即压缩后的特征数,X是数据，矩阵形式
    n_samples,n_features=X.shape
    mean=np.array([np.mean(X[:,i]) for i in range(n_features)])
    norm_X=X-mean  #正则化处理
    scatter_martrix=np.dot(np.transpose(norm_X),norm_X)  #散度矩阵，np.dot（）为点乘，即内积
    eig_val,eig_vec=np.linalg.eig(scatter_martrix)
    #计算特征值和特征向量  单词eigen:特征的，固有的
    eig_pairs=[(np.abs(eig_val[i]),eig_vec[:,i]) for i in range(n_features)]
    #将特征向量排序并选取k个最高值
    eig_pairs.sort(reverse=True)
    feature=np.array([ele[1] for ele in eig_pairs[:k]])  #有一点疑惑
    data=np.dot(norm_X,np.transpose(feature))  #得到新的数据,即降维后的数据
    return  data
iris=datasets.load_iris()  #使用sklearn导入iris
data=np.mat(data)
print(pca(data,2))
